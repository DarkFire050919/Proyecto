# Proyecto_P2.py â€“ Clasificador de Spam con PyTorch

## ğŸ“„ DescripciÃ³n general
Este script implementa un clasificador binario de correos electrÃ³nicos spam/notâ€‘spam utilizando una red neuronal densa (fullyâ€‘connected) construida con **PyTorch**. El conjunto de datos proviene del repositorio **UCI Spambase** ([UCI Machine Learning Repository â€“ Spambase](https://archive.ics.uci.edu/ml/datasets/spambase)).

El flujo de trabajo incluye:
1. **Carga y preâ€‘procesamiento** de los datos (train/test split, normalizaciÃ³n).  
2. DefiniciÃ³n de la arquitectura de la red neuronal con capas lineales, activaciones ReLU, dropout y salida sigmoide.  
3. **Entrenamiento** mediante el optimizador Adam y la funciÃ³n de pÃ©rdida binaria crossâ€‘entropy (BCELoss).  
4. **EvaluaciÃ³n** en el conjunto de prueba con precisiÃ³n (accuracy).  
5. **VisualizaciÃ³n** de la curva de pÃ©rdida y de precisiÃ³n a lo largo de las Ã©pocas.

---

## ğŸ› ï¸ Requisitos e instalaciÃ³n
```bash
# Entorno de Python (>=3.8)
pip install torch torchvision matplotlib scikit-learn ucimlrepo
```
- **torch** â€“ Framework de deep learning.
- **matplotlib** â€“ GeneraciÃ³n de grÃ¡ficas.
- **scikitâ€‘learn** â€“ Funciones auxiliares (train_test_split, StandardScaler, accuracy_score).
- **ucimlrepo** â€“ Cliente para descargar el dataset Spambase.

---

## âš™ï¸ ConfiguraciÃ³n de hiperparÃ¡metros (justificaciÃ³n)
| ParÃ¡metro | Valor | Razonamiento |
|-----------|-------|--------------|
| `LR` (learning rate) | `0.001` | Valor estÃ¡ndar para el optimizador Adam; evita oscilaciones y garantiza convergencia estable. |
| `EPOCHS` | `500` | El dataset tiene 57 caracterÃ­sticas; se requiere suficiente nÃºmero de iteraciones para una convergencia suave. |
| `HIDDEN_1` | `128` | Expande la dimensionalidad de entrada (57) a 128 para capturar combinaciones no lineales complejas. |
| `HIDDEN_2` | `64` | ReducciÃ³n progresiva (embudo) que sintetiza la informaciÃ³n aprendida en la capa anterior. |
| `DROPOUT_RATE` | `0.4` | Apaga el 40â€¯% de neuronas durante el entrenamiento, reduciendo el riesgo de overâ€‘fitting. |

---

## ğŸ“‚ Funciones principales
### `load_and_preprocess_data()`
- Descarga el dataset Spambase mediante `fetch_ucirepo(id=94)`.
- Separa **features** (`X`) y **etiquetas** (`y`).
- Divide los datos en *train* (80â€¯%) y *test* (20â€¯%) con `train_test_split` (semilla `random_state=42`).
- Normaliza las caracterÃ­sticas usando `StandardScaler` (media 0, varianza 1).  
- Convierte los arrays a tensores `torch.float` y los devuelve en un diccionario.

### `SpamClassifier(nn.Module)`
```text
Input (57) â†’ Linear(57, 128) â†’ ReLU â†’ Dropout(0.4)
      â†’ Linear(128, 64) â†’ ReLU â†’ Linear(64, 1) â†’ Sigmoid
```
- **Salida**: probabilidad entre 0 y 1.
- **RegularizaciÃ³n**: dropout activo solo en modo *train*.

---

## ğŸ‹ï¸â€â™‚ï¸ Proceso de entrenamiento
```python
model = SpamClassifier()
criterion = nn.BCELoss()
optimizer = optim.Adam(model.parameters(), lr=LR)

for epoch in range(EPOCHS):
    model.train()
    optimizer.zero_grad()
    outputs = model(data['X_train'])
    loss = criterion(outputs, data['y_train'])
    loss.backward()
    optimizer.step()
    # Registro de loss y accuracy
```
- Cada 50 Ã©pocas se imprime la pÃ©rdida y la precisiÃ³n de entrenamiento.
- Se almacenan `loss_history` y `accuracy_history` para graficar.

---

## ğŸ“Š EvaluaciÃ³n final
```python
model.eval()
with torch.no_grad():
    test_outputs = model(data['X_test'])
    test_predicted = (test_outputs > 0.5).float()
    final_acc = accuracy_score(data['y_test'], test_predicted)
```
- **PrecisiÃ³n final** (accuracy) se muestra en consola, por ejemplo:
```
Accuracy Final en Test Set: 93.45%
```
*(El valor exacto dependerÃ¡ de la semilla y de los parÃ¡metros de entrenamiento.)*

---

## ğŸ“ˆ Visualizaciones
Dos subâ€‘grÃ¡ficas se generan con **matplotlib**:
1. **Curva de pÃ©rdida (Loss) vs. Ã©pocas** â€“ muestra la disminuciÃ³n de la funciÃ³n objetivo.
2. **Curva de precisiÃ³n (Accuracy) vs. Ã©pocas** â€“ evidencia el progreso del modelo durante el entrenamiento.
```python
plt.subplot(1, 2, 1)  # Loss
plt.subplot(1, 2, 2)  # Accuracy
plt.show()
```
> Si se desea guardar la figura, descomentar la lÃ­nea `plt.savefig('resultados_entrenamiento.png')`.

---

## ğŸš€ Uso rÃ¡pido
```bash
python Proyecto_P2.py
```
El script entrenarÃ¡ el modelo y mostrarÃ¡ por pantalla:
- Progreso por cada 50 Ã©pocas (Loss y Accuracy de entrenamiento).
- PrecisiÃ³n final en el conjunto de prueba.
- Ventana de visualizaciÃ³n con las dos curvas.

---

## ğŸ“š Referencias y recursos
- **UCI Machine Learning Repository â€“ Spambase**: https://archive.ics.uci.edu/ml/datasets/spambase
- **PyTorch Documentation**: https://pytorch.org/docs/stable/index.html
- **scikitâ€‘learn**: https://scikit-learn.org/
- **ucimlrepo** (Python client): https://pypi.org/project/ucimlrepo/

---

## âœï¸ Comentarios y posibles extensiones
- **Crossâ€‘validation** para una estimaciÃ³n mÃ¡s robusta del rendimiento.
- **Ajuste de hiperparÃ¡metros** mediante bÃºsqueda en cuadrÃ­cula o algoritmos evolutivos.
- **Persistencia del modelo** (`torch.save`) para reutilizar el clasificador sin reâ€‘entrenar.
- **MÃ©tricas adicionales** (precision, recall, F1â€‘score, ROCâ€‘AUC) para evaluar el sesgo del dataset.

*Fin del documento.*